from dotenv import load_dotenv
load_dotenv()

import os
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ====== ç”»é¢è¨­å®š ======
st.set_page_config(page_title="3äººã®å¥åº·å°‚é–€å®¶ã«ç›¸è«‡ã—ã‚ˆã†", page_icon="ğŸ§‘â€âš•ï¸")

# ====== LLM è¨­å®š ======
MODEL_NAME = "gpt-4o-mini"
TEMPERATURE = 0.3
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯.envãƒ•ã‚¡ã‚¤ãƒ«ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

llm = ChatOpenAI(
    model=MODEL_NAME,
    temperature=TEMPERATURE,
    openai_api_key=OPENAI_API_KEY
)

# ====== å°‚é–€å®¶ã”ã¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ ======
EXPERT_PROMPTS = {
    "é£Ÿã®å°‚é–€å®¶ï¼ˆç®¡ç†æ „é¤Šå£«ï¼‰": (
        "ã‚ãªãŸã¯æ—¥æœ¬ã®é£Ÿæ–‡åŒ–ãƒ»æ „é¤Šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«ç²¾é€šã—ãŸç®¡ç†æ „é¤Šå£«ã§ã™ã€‚"
        "ç›¸è«‡è€…ã®çŠ¶æ³ã‹ã‚‰å®‰å…¨ã§å®Ÿè¡Œã—ã‚„ã™ã„é£Ÿäº‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        "å…·ä½“çš„ãªé£Ÿå“ã‚„é‡ã€ä»£æ›¿æ¡ˆã€æ³¨æ„ç‚¹ã‚’ç®‡æ¡æ›¸ãã§ç¤ºã—ã€åŒ»å­¦çš„è¨ºæ–­ã¯é¿ã‘ã¾ã™ã€‚"
    ),
    "ç¡çœ ã®å°‚é–€å®¶ï¼ˆç¡çœ è¡›ç”Ÿï¼‰": (
        "ã‚ãªãŸã¯ç¡çœ è¡›ç”Ÿã¨æ¦‚æ—¥ãƒªã‚ºãƒ ã«è©³ã—ã„ç¡çœ ã®å°‚é–€å®¶ã§ã™ã€‚"
        "å°±å¯/èµ·åºŠã®ä¸€è²«æ€§ã€å…‰æ›éœ²ã€ã‚«ãƒ•ã‚§ã‚¤ãƒ³ã€æ˜¼å¯ã€å°±å¯å‰ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚’ä¸­å¿ƒã«ã€"
        "ä»Šæ—¥ã‹ã‚‰è©¦ã›ã‚‹æ‰‹é †ã‚’å„ªå…ˆåº¦é †ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚åŒ»å­¦çš„è¨ºæ–­ã¯é¿ã‘ã¾ã™ã€‚"
    ),
    "é‹å‹•ã®å°‚é–€å®¶ï¼ˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼‰": (
        "ã‚ãªãŸã¯è² è·ç®¡ç†ã¨ãƒ•ã‚©ãƒ¼ãƒ ã‚’é‡è¦–ã™ã‚‹é‹å‹•ã®å°‚é–€å®¶ï¼ˆS&Cã‚³ãƒ¼ãƒï¼‰ã§ã™ã€‚"
        "ç›®çš„ãƒ»ä½“åŠ›ãƒ¬ãƒ™ãƒ«ã‚’æƒ³å®šã—ã¤ã¤ã€å®‰å…¨ç¬¬ä¸€ã§é€±ã‚ãŸã‚Šã®é »åº¦ã€ç¨®ç›®ä¾‹ã€æ™‚é–“ã€"
        "å¼·åº¦ç›®å®‰(RPEç­‰)ã€æ³¨æ„ç‚¹ã‚’å…·ä½“çš„ã«ç¤ºã—ã¦ãã ã•ã„ã€‚åŒ»ç™‚çš„è¨ºæ–­ã¯é¿ã‘ã¾ã™ã€‚"
    ),
}

# ====== ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰é–¢æ•° ======
def build_chain(system_msg: str):
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_msg),
        ("human", "{question}")
    ])
    return prompt | llm | StrOutputParser()

# ====== LLMå¿œç­”å–å¾—é–¢æ•° ======
def get_llm_response(question: str, expert: str) -> str:
    chain = build_chain(EXPERT_PROMPTS[expert])
    return chain.invoke({"question": question})

# ====== ç”»é¢ï¼šæ¦‚è¦ & æ“ä½œèª¬æ˜ ======
st.title("ğŸ§‘â€âš•ï¸ 3äººã®å°‚é–€å®¶ã«ç›¸è«‡ã—ã‚ˆã†")
st.markdown(
    "ã“ã®ã‚¢ãƒ—ãƒªã¯ã€**é£Ÿãƒ»ç¡çœ ãƒ»é‹å‹•**ã®3åˆ†é‡ã®å°‚é–€å®¶ã‹ã‚‰ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã‚‚ã‚‰ãˆã‚‹AIã‚¢ãƒ—ãƒªã§ã™ã€‚  \n"
    "ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã‚’é¸ã³ã€ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦é€ä¿¡ã™ã‚‹ã¨ã€å°‚é–€å®¶ã®è¦–ç‚¹ã§å›ç­”ã—ã¾ã™ã€‚"
)
st.divider()

st.subheader("ğŸ”˜ æ“ä½œæ–¹æ³•")
st.markdown(
    "1. ã‚ãªãŸã®æ‚©ã¿ã«ã‚ã£ãŸ**å°‚é–€å®¶**ã‚’é¸ã¶\n"
    "2. **ç›¸è«‡å†…å®¹**ã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šæœ€è¿‘å¯ã¤ããŒæ‚ªã„ï¼æ¸›é‡ã—ãŸã„ï¼è‚©ã“ã‚Šã‚’æ”¹å–„ã—ãŸã„ ãªã©ï¼‰\n"
    "3. **é€ä¿¡**ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€å°‚é–€å®¶ãŒå›ç­”ã—ã¾ã™\n"
)
st.warning("â€» æœ¬ã‚¢ãƒ—ãƒªã®å›ç­”ã¯ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ã‚ã‚Šã€åŒ»ç™‚è¡Œç‚ºã‚„è¨ºæ–­ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
st.divider()

# ====== å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ  ======
st.subheader("ğŸ“ ç›¸è«‡ãƒ•ã‚©ãƒ¼ãƒ ")
with st.form(key="consult_form"):
    expert = st.radio(
        "ã©ã®å°‚é–€å®¶ã«ç›¸è«‡ã—ã¾ã™ã‹ï¼Ÿ",
        list(EXPERT_PROMPTS.keys()),
        index=0,
        horizontal=True
    )
    question = st.text_area(
        "ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›",
        height=160,
        placeholder="ä¾‹ï¼‰å¤œä¸­ã«ä½•åº¦ã‚‚ç›®ãŒè¦šã‚ã¾ã™ã€‚æœã‚¹ãƒƒã‚­ãƒªèµ·ãã‚‹ãŸã‚ã«ä»Šæ—¥ã‹ã‚‰ã§ãã‚‹ã“ã¨ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
    )
    submitted = st.form_submit_button("é€ä¿¡")

# ====== å®Ÿè¡Œ & çµæœè¡¨ç¤º ======
if submitted:
    if not question.strip():
        st.warning("ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("å›ç­”ã‚’ä½œæˆä¸­â€¦"):
            answer = get_llm_response(question, expert)
        st.divider()
        st.info(f"**é¸æŠã•ã‚ŒãŸå°‚é–€å®¶**ï¼š{expert}\n\n**ç›¸è«‡å†…å®¹**ï¼š{question}")

        st.markdown("#### å›ç­”")
        st.markdown(answer)
        st.divider()
        with st.expander("ğŸ’¡ ä½¿ã‚ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º"):
            st.code(EXPERT_PROMPTS[expert], language="markdown")